---
title: "Practicum Portfolio"
author: "Natalie Goulett"
date: "2025-02-02"
format: html
editor: visual
code-fold: TRUE
echo: FALSE
---

# Introduction

This is my walk-through of my practicum project.

# Background

\[Discuss best practices when Designing A REDCap Project, writing functions, conducting Data Analysis, etc.\]

## Getting REDCap Data

To explore and analyze my REDCap data, I must first send it to my statistical software, in this case R and RStudio. One option is to log into my REDCap project and manually download the data from REDCap to a local file, then read it into R. This method is unfavorable because it 1. Requires my input every time the REDCap data is updated, and 2. Is prone to security issues, as my protected data could be compromised via the network it is downloaded on or through mishandling of the downloaded file(s). Instead, I will utilize the functionality of REDCap’s Application Programming Interface (API) to securely send my encrypted data to R. \[define API\]

In R, I’ll first install some necessary packages, primarily the Rosyverse packages by Dr. Brandon Edward Rose. `RosyREDCap` will be used to download the encrypted REDCap data into a database (DB) object in my R environment. Because many of these packages are actively being developed, I use a dedicated .r script containing update commands to easily and frequently update my installations:

```{r}
#| label: install-update-packages

# Install the remotes package (stable version) if not already installed
# Note: RosyREDCap and RosyDB functions have been reorganized into the RosyApp and REDCapSync packages.
install.packages("remotes")
library(remotes)
# remotes::install_github("brandonerose/RosyREDCap")
# remotes::install_github("brandonerose/RosyREDCap",upgrade = "never") # old
remotes::install_github("brandonerose/RosyDB",upgrade = "never")
remotes::install_github("brandonerose/RosyApp",upgrade = "never")
remotes::install_github("brandonerose/RosyUtils",upgrade = "never")
remotes::install_github("brandonerose/RosyDev",upgrade = "never")
remotes::install_github("brandonerose/Rosyverse",upgrade = "never")
remotes::install_github("thecodingdocs/REDCapSync",upgrade = "never")

# Install devtools to update each package using current GitHub versions
install.packages("devtools")
devtools::load_all()

# The development version is available via pak:
install.packages("pak")
pak::pak("thecodingdocs/REDCapSync")
# error in pak subprocess

# Restart R session if desired
# .rs.restartR()

# load desired packages
library(REDCapSync)
library(RosyUtils)
library(RosyDev)
library(RosyDB)

```

To set up an easy and secure framework to download REDCap data via the API, I'll first save my API token to my `.Renviron` script and give it a name. This file sets up the environment to my specifications every time I start R.\*\*\*

```{r}
#| label: edit-renviron

# Open the .Renviron script
usethis::edit_r_environ()

# Within this script, name the API token (I named mine `PRAC_token`) and 
#   replace YourNeverShareToken with the 32-digit token:
PRAC_token = "YourNeverShareToken"

```

Now, in a separate script, I can set up the DB object using RosyREDCap. I'll provide the `token_name` from my `.Renviron` file as well as my URI for my REDCap database.

```{r}
#| label: setup-db

# load required packages
library("REDCapSync")
# library("RosyREDCap")
# library("RosyDB")

# save projects to global environment
# note: setup functions from RosyREDCap were replaced with the REDCapSync functions
# projects <- RosyDB::get_projects()

DB <-setup_project(
  short_name = "PRAC",
  dir_path = getwd() %>% normalizePath(),
  # for security, the API token is defined in the .Renviron file
  token_name = "PRAC_token",
  redcap_base = "https://redcap.fiu.edu/",
  merge_form_name = "patient"
)
# After the above code has been ran once, we can quickly load the project using
#   the sync_project() function
DB <- sync_project(DB)

# Note: these functions are outdated
# DB <- update_RosyREDCap(DB)
# DB$metadata %>% RosyUtils::add_list_to_global()
# Uncomment to merge repeating forms into one long data.frame:
# DB <- add_forms_transformation(DB,forms_transformation =  default_forms_transformation(DB))
# DB <- clean_DB(DB)
# DB$data %>% RosyUtils::add_list_to_global()

```

# Development

## Bug Fixes

The Rosyverse packages are still under development. As a contributor, I aim to help identify and resolve code issues while I use these evolving pre-CRAN packages on a Windows OS. I encountered an error when running the setup_RosyREDCap while using a Windows computer: "'\U' used without hex digits in character string". The path generated by this function had been written using "file://" instead of "file:\\" due to a bug in a dependency of the `bullet_in_console` function from the `RosyUtils` package. This bug had gone undetected as it had previously only been used on Mac OS, which uses "/" in its file path formats. This and similar file path-related bugs were identified and fixed by Dr. Rose to support Windows compatibility.

The `setup_RosyREDCap` creates a `DB` object containing data.frames of the data and metadata from each instrument of the REDCap project. The `clean_DB` function converts columns in `DB` to their correct data types and labels them according to the REDCap metadata. If the column class is factor, it generates factor levels for unique values in the column.

## Visualization Functions

See the_whole_game.R.

## Unit Testing

# Patient Data <!--# Simulation--> Analysis!

We want to analyze how variables such as ethnicity, sarcoma category, and chemotherapy treatment influence a patients’ progression-free survival (Note: we are using OS for now, but can add PFS once we have variables for progression). Progression-free survival is defined as the time from (diagnosis or start of treatment?) until first evidence of disease progression or death. Progression events in my REDCap database include the date when a (metastasis, increase in tumor size, recurrence,) or death is observed. (Further variables could be added to account for more types of progression.)

Synthetic patient data based on the real patient data from the Pan-Sarcoma database must be generated in a way that preserves the relationships between predictors (sex, ethnicity, sarcoma category, drug type, etc) and survival time *t*. We also need to account for the fact that patients may be lost to follow-up before the events can be observed.

## Model Selection

To accomplish this, two parametric survival models must be fit to the original data to determine the distribution parameters and regression coefficients – one that models the time-to-event behavior and one that models the censoring behavior among observed patients, and how both events are influenced by predictor variables (covariates).

We will first fit a semi-parametric model, a Cox-proportional Hazard model, to understand the relationship between predictors and overall survival without assuming a specific baseline hazard. Because the `survsim` function requires model parameters to simulate data, we will also fit parametric models assuming different distributions (Weibull, log normal, and log logistic) using the `survreg` function and compare them using AIC. The model with the lowest AIC explains the most variation of the outcome and thus is the most accurate model to use for data simulation.

```{r}

# Load necessary libraries
library(tidyverse)
library(lubridate)
library(survival)

# Step 1: Import real PSDB REDCap data into DB object

# create data.frame object `DF`
DF <- patient %>% merge(sarcoma, by = "record_id")
# *** why must we clean this new DF even after cleaning DB? Because every merge drops some attributes
DF <- clean_DF(DF, DB$metadata$fields)

# Convert date columns to date format
DF <- DF %>%
  mutate(
    date_of_diagnosis = ymd(date_of_diagnosis),
    date_of_last_contact = ymd(date_of_last_contact)
  )

# Calculate overall survival in days
DF <- DF %>%
  mutate(
    time_to_event = as.numeric(difftime(
      date_of_last_contact, 
      date_of_diagnosis, 
      units = "days"
    )),
    # Define the event indicator (1 = Deceased, 0 = Alive)
    event = ifelse(status_at_last_contact == 1, 1, 0)
  )

```

```{r}
#| eval: FALSE

# Step 2: Fit Survival Models
# Note: These are variables I currently have in REDCap that may be significant. 
# Subject matter expert can inform initial predictor choice.
# We can fit a cox model with ALL predictors and perform backwards 
#   selection to remove the least significant predictor (highest p-value),
#   re-fit the model with remaining predictors, repeat until all are significant (p < 0.05).

# Fit Non-parametric model Cox proportional hazards model
cox_model <- coxph(
  Surv(time_to_event, event) ~ sex_at_birth + is_hispanic + grade_at_diagnosis +
    metastatic_at_diagnosis + drug_1 + sarcoma_category + sarcoma_type,
  data = DF
)
summary(cox_model)
# check VIF of each variable. Remove variable with highest VIF (> 10) and re fit
#   model until all VIFs less than 10 to ensure no multicollinearity exists.
VIF(cox_model)

# Fit parametric (Weibull, log-normal, and log-logistic) models using the
#  significant predictors from the cox_model (p < 0.05)
weibull_model <- survreg(
  Surv(time_to_event, event) ~
    sex_at_birth +
    is_hispanic +
    grade_at_diagnosis +
    metastatic_at_diagnosis +
    drug_1 +
    sarcoma_category +
    sarcoma_type,
  data = DF,
  dist = "weibull"
)

lognorm_model <- survreg(
  Surv(time_to_event, event) ~
    sex_at_birth +
    is_hispanic +
    grade_at_diagnosis +
    metastatic_at_diagnosis +
    drug_1 +
    sarcoma_category +
    sarcoma_type,
  data = DF,
  dist = "lognormal"
)

loglogis_model <- survreg(
  Surv(time_to_event, event) ~
    sex_at_birth +
    is_hispanic +
    grade_at_diagnosis +
    metastatic_at_diagnosis +
    drug_1 +
    sarcoma_category +
    sarcoma_type,
  data = DF,
  dist = "loglogistic"
)

# Compare 3 parametric models using AIC and choose model with the lowest AIC
AIC(weibull_model, lognorm_model, loglogis_model)

# Further diagnostics - 
# likelihood ratio test
anova(weibull_model, lognorm_model, loglogis_model, test = "LRT")
# log likelihood comparison (higher = better fit)
logLik(weibull_model)
logLik(lognorm_model)
logLik(loglogis_model)

```

Note: We got IRB approval and won't need to simulate data anymore! Stay tuned for analysis of real de-identified data.

# Data Analysis

Coming soon: exploring, analyzing, and visualizing simulated data with our functions!

# Goals

-   Create a one-script R package of my functions using golem
-   Debug Rosyverse
